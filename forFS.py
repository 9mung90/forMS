# -*- coding: utf-8 -*-
import sys
import re
import torch
import logging

# íŠ¸ëœìŠ¤í¬ë¨¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ (Hugging Face)
from transformers import AutoTokenizer, BertForSequenceClassification

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸° (ë°ëª¨ í™”ë©´ì„ ê¹”ë”í•˜ê²Œ í•˜ê¸° ìœ„í•¨)
logging.getLogger("transformers").setLevel(logging.ERROR)

# =============================================================================
# 1. ì„¤ì • ë° ë°ì´í„° (ìš•ì„¤ ì‚¬ì „, í”¼ë“œë°± ë©”ì‹œì§€)
# =============================================================================

# [ì„¤ì •] ì‚¬ìš©í•  ëª¨ë¸ (Smilegate AIì˜ UnSmile ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸)
MODEL_NAME = "smilegate-ai/kor_unsmile"

# [ë°ì´í„°] í˜ì˜¤ ìœ í˜• ë¼ë²¨ (ëª¨ë¸ ì¶œë ¥ ìˆœì„œì™€ ë™ì¼í•´ì•¼ í•¨)
LABELS = [
    "ì—¬ì„±/ê°€ì¡±", "ë‚¨ì„±", "ì„±ì†Œìˆ˜ì", "ì¸ì¢…/êµ­ì ",
    "ì—°ë ¹", "ì§€ì—­", "ì¢…êµ", "ê¸°íƒ€ í˜ì˜¤",
    "ì•…í”Œ/ìš•ì„¤", "clean"
]

# [ë°ì´í„°] ìš•ì„¤/ë¹„í•˜ í‘œí˜„ ì¹˜í™˜ ì‚¬ì „ (Badword Masking)
# ì‹œì—°ì„ ìœ„í•´ ëŒ€í‘œì ì¸ ë‹¨ì–´ë“¤ë¡œ êµ¬ì„±
BADWORD_DICT = {
    "ë³‘ì‹ ": "ì‚¬ëŒ",
    "ã…‚ã……": "ì‚¬ëŒ",
    "ë¯¸ì¹œë†ˆ": "ì‚¬ëŒ",
    "êº¼ì ¸": "ì €ë¦¬ ê°€ ì¤˜",
    "ì“°ë ˆê¸°": "ì¢‹ì§€ ì•Šì€ í–‰ë™ì„ í•˜ëŠ” ì‚¬ëŒ",
    "ì§±ê¹¨": "ì¤‘êµ­ì¸",
    "ìª½ë°œì´": "ì¼ë³¸ì¸",
    "ëŠê¸ˆë§ˆ": "ê°€ì¡±",
    "ê°œê°™ì€": "ë‚˜ìœ",
}

# [ë°ì´í„°] í˜ì˜¤ ìœ í˜•ë³„ ìˆœí™” ê°€ì´ë“œ (Rewrite Suggestion)
FEEDBACK_DICT = {
    "ì—¬ì„±/ê°€ì¡±": "ì„±ë³„ì´ë‚˜ ê°€ì¡± ì „ì²´ë¥¼ ì¼ë°˜í™”í•˜ê¸°ë³´ë‹¤ëŠ”, íŠ¹ì • ìƒí™©ì´ë‚˜ í–‰ë™ì— ëŒ€í•´ ì„¤ëª…í•˜ëŠ” í‘œí˜„ì´ ì¢‹ìŠµë‹ˆë‹¤.",
    "ë‚¨ì„±": "íŠ¹ì • ì„±ë³„ì„ ì‹¸ì¡ì•„ ë¹„ë‚œí•˜ê¸°ë³´ë‹¤ëŠ”, ë¬¸ì œë¼ê³  ëŠë‚€ í–‰ë™ì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ ì´ì•¼ê¸°í•´ë³´ì„¸ìš”.",
    "ì„±ì†Œìˆ˜ì": "ì„±ì  ì§€í–¥ì´ë‚˜ ì •ì²´ì„±ì„ ê³µê²©í•˜ê¸°ë³´ë‹¤, ì„œë¡œì˜ ë‹¤ë¦„ì„ ì¸ì •í•˜ê³  ì¡´ì¤‘í•˜ëŠ” í‘œí˜„ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.",
    "ì¸ì¢…/êµ­ì ": "êµ­ì ì´ë‚˜ ì¸ì¢… ì „ì²´ë¥¼ ë¹„í•˜í•˜ì§€ ë§ê³ , ê°ê´€ì ì¸ ìƒí™©ì´ë‚˜ ì œë„ë¥¼ ì„¤ëª…í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.",
    "ì—°ë ¹": "ë‚˜ì´ë§Œì„ ì´ìœ ë¡œ ë¹„í•˜í•˜ê¸°ë³´ë‹¤ëŠ”, ì„¸ëŒ€ ê°„ ì°¨ì´ë¥¼ ì´í•´í•˜ë ¤ëŠ” íƒœë„ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
    "ì§€ì—­": "íŠ¹ì • ì§€ì—­ ì‚¬ëŒ ì „ì²´ë¥¼ ë§¤ë„í•˜ê¸°ë³´ë‹¤ëŠ”, ë³¸ì¸ì´ ê²ªì€ ê°œë³„ì ì¸ ê²½í—˜ìœ¼ë¡œ êµ­í•œí•˜ì—¬ í‘œí˜„í•´ ì£¼ì„¸ìš”.",
    "ì¢…êµ": "ì‹ ì•™ ìì²´ë¥¼ ê³µê²©í•˜ê¸°ë³´ë‹¤ëŠ”, ë™ì˜í•˜ì§€ ì•ŠëŠ” ì˜ê²¬ì— ëŒ€í•´ ë…¼ë¦¬ì ìœ¼ë¡œ ë°˜ë°•í•´ ë³´ì„¸ìš”.",
    "ê¸°íƒ€ í˜ì˜¤": "ì§‘ë‹¨ ì „ì²´ë¥¼ í–¥í•œ í˜ì˜¤ í‘œí˜„ì€ ì§€ì–‘í•˜ê³ , êµ¬ì²´ì ì¸ ì‚¬ì‹¤ì— ê¸°ë°˜í•´ ê±´ì „í•˜ê²Œ ëŒ€í™”í•´ ì£¼ì„¸ìš”.",
    "ì•…í”Œ/ìš•ì„¤": "ê°•í•œ ê°ì •ì´ ë“¤ ë•ŒëŠ” ì ì‹œ ì§„ì •í•˜ê³ , ìš•ì„¤ ëŒ€ì‹  ë¶ˆë§Œ ì‚¬í•­ì„ êµ¬ì²´ì ìœ¼ë¡œ ì ì–´ë³´ì„¸ìš”.",
    "clean": "ìƒëŒ€ë¥¼ ì¡´ì¤‘í•˜ëŠ” í‘œí˜„ì…ë‹ˆë‹¤. ì§€ê¸ˆì²˜ëŸ¼ ê±´ê°•í•œ ì˜¨ë¼ì¸ ì†Œí†µì„ ì´ì–´ê°€ ì£¼ì„¸ìš”."
}


# ì •ê·œì‹ íŒ¨í„´ ì»´íŒŒì¼ (ì¹˜í™˜ ì†ë„ í–¥ìƒ)
def build_badword_patterns(badword_dict):
    patterns = {}
    for bad in badword_dict.keys():
        # ìš•ì„¤ ë’¤ì— ë¶™ëŠ” ì¡°ì‚¬(ì´, ê°€, ì€, ëŠ” ë“±)ë¥¼ ìœ ì—°í•˜ê²Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì •ê·œì‹
        escaped = re.escape(bad)
        pattern = re.compile(rf"({escaped})([ì´ê°€ì€ëŠ”ì„ë¥¼]*)")
        patterns[bad] = pattern
    return patterns


BADWORD_PATTERNS = build_badword_patterns(BADWORD_DICT)


# =============================================================================
# 2. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ (ëª¨ë¸ ë¡œë“œ, ì „ì²˜ë¦¬, ì˜ˆì¸¡)
# =============================================================================

def load_system():
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    print("\n[ì‹œìŠ¤í…œ] AI ëª¨ë¸ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”)")
    device = "cuda" if torch.cuda.is_available() else "cpu"

    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        model = BertForSequenceClassification.from_pretrained(MODEL_NAME)
        model.to(device)
        model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        print(f"[ì‹œìŠ¤í…œ] ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ê°€ì† ì¥ì¹˜: {device})")
        return model, tokenizer, device
    except Exception as e:
        print(f"\n[ì˜¤ë¥˜] ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\nì›ì¸: {e}")
        print("ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ 'pip install transformers torch' ì„¤ì¹˜ ì—¬ë¶€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        sys.exit(1)


def replace_badwords_func(text):
    """ë¬¸ì¥ ë‚´ ìš•ì„¤ì„ ìˆœí™”ëœ í‘œí˜„ìœ¼ë¡œ ì¹˜í™˜í•©ë‹ˆë‹¤."""
    cleaned_text = text
    logs = []

    # ê¸´ ë‹¨ì–´ë¶€í„° ì¹˜í™˜í•˜ê¸° ìœ„í•´ ì •ë ¬ (ì˜ˆ: 'ê°œ'ë³´ë‹¤ 'ê°œìƒˆë¼'ë¥¼ ë¨¼ì € ì°¾ìŒ)
    sorted_keys = sorted(BADWORD_DICT.keys(), key=len, reverse=True)

    for bad in sorted_keys:
        pattern = BADWORD_PATTERNS[bad]
        replacement = BADWORD_DICT[bad]

        def _sub_func(match):
            word = match.group(1)
            tail = match.group(2) or ""  # ì¡°ì‚¬
            # ë¡œê·¸ì— ê¸°ë¡ (ì˜ˆ: 'ë³‘ì‹ ' -> 'ì‚¬ëŒ')
            logs.append(f"'{word}' -> '{replacement}'")
            return replacement + tail

        cleaned_text, _ = pattern.subn(_sub_func, cleaned_text)

    return cleaned_text, logs


def analyze_sentence(text, model, tokenizer, device):
    """ë¬¸ì¥ì„ ë¶„ì„í•˜ì—¬ í˜ì˜¤ ìœ í˜•, ìˆœí™” ë¬¸ì¥, í”¼ë“œë°±ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""

    # 1. ëª¨ë¸ ì˜ˆì¸¡ (Inference)
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        # ë¡œì§“(Logits)ì„ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ 0~1 ì‚¬ì´ í™•ë¥ ê°’ ë„ì¶œ
        probs = torch.sigmoid(outputs.logits[0])

    probs_list = probs.cpu().tolist()

    # 2. ì„ê³„ê°’(Threshold) 0.5 ì´ìƒì¸ ë¼ë²¨ ì¶”ì¶œ
    detected_labels = []
    label_scores = {}  # í”¼ë“œë°± ì„ ì •ì„ ìœ„í•´ ì ìˆ˜ ì €ì¥

    for i, score in enumerate(probs_list):
        if score >= 0.5:
            label_name = LABELS[i]
            detected_labels.append(label_name)
            label_scores[label_name] = score

    # 3. ìš•ì„¤ ì¹˜í™˜ ìˆ˜í–‰
    cleaned_text, replace_logs = replace_badwords_func(text)

    # 4. ë§ì¶¤í˜• í”¼ë“œë°± ì„ ì •
    # ê°ì§€ëœ í˜ì˜¤ ë¼ë²¨ ì¤‘ 'ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€' ë¼ë²¨ì˜ í”¼ë“œë°±ì„ ëŒ€í‘œë¡œ ë³´ì—¬ì¤Œ
    main_feedback = ""

    # cleanì´ ì•„ë‹ˆë©´ì„œ ê°ì§€ëœ ë¼ë²¨ì´ ìˆëŠ” ê²½ìš°
    hate_labels = [l for l in detected_labels if l != "clean"]

    if hate_labels:
        # í˜ì˜¤ ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ë¼ë²¨ ì°¾ê¸°
        top_label = max(hate_labels, key=lambda l: label_scores[l])
        main_feedback = FEEDBACK_DICT.get(top_label, "ìƒëŒ€ë¥¼ ì¡´ì¤‘í•˜ëŠ” ê³ ìš´ ë§ì„ ì¨ì£¼ì„¸ìš”.")
    elif "clean" in detected_labels:
        main_feedback = FEEDBACK_DICT["clean"]
    else:
        # ì•„ë¬´ê²ƒë„ ê°ì§€ë˜ì§€ ì•Šì•˜ê±°ë‚˜(Threshold ë¯¸ë§Œ) ì• ë§¤í•œ ê²½ìš°
        detected_labels.append("ì •ìƒ(Clean)")
        main_feedback = FEEDBACK_DICT["clean"]

    return {
        "original": text,
        "cleaned": cleaned_text,
        "labels": detected_labels,
        "replace_logs": replace_logs,
        "feedback": main_feedback
    }


# =============================================================================
# 3. ë©”ì¸ ì‹¤í–‰ ë£¨í”„ (ë°ëª¨ UI)
# =============================================================================

def run_demo():
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer, device = load_system()

    print("\n" + "=" * 70)
    print("      [ ê¸°ê³„í•™ìŠµ 8ì¡°: ë¬¸ë§¥ ê¸°ë°˜ ì‹¤ì‹œê°„ í˜ì˜¤ í‘œí˜„ ë‹¤ì¤‘ ë¶„ë¥˜ ì‹œìŠ¤í…œ ]")
    print("=" * 70)
    print(" â€» ì¢…ë£Œí•˜ë ¤ë©´ 'q' ë˜ëŠ” 'exit'ì„ ì…ë ¥í•˜ì„¸ìš”.\n")

    while True:
        try:
            # ì‚¬ìš©ì ì…ë ¥
            print("-" * 70)
            user_input = input("ğŸ“ ë¶„ì„í•  ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”: ")

            # ì¢…ë£Œ ì¡°ê±´
            if user_input.strip().lower() in ['q', 'exit', 'quit']:
                print("\n[ì‹œìŠ¤í…œ] ë°ëª¨ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.")
                break

            # ë¹ˆ ì…ë ¥ ì²˜ë¦¬
            if not user_input.strip():
                continue

            # ë¶„ì„ ìˆ˜í–‰
            result = analyze_sentence(user_input, model, tokenizer, device)

            # --- ê²°ê³¼ ì¶œë ¥ í™”ë©´ (ê°€ë…ì„± ìµœì í™”) ---
            print("\n   [ ë¶„ì„ ê²°ê³¼ ]")

            # 1. ì›ë¬¸
            print(f"   â–¶ ì›ë¬¸ ë¬¸ì¥ :  {result['original']}")

            # 2. ìˆœí™” (ì¹˜í™˜ëœ ê²½ìš°ì—ë§Œ í‘œì‹œ)
            if result['replace_logs']:
                print(f"   â–¶ ìˆœí™” ë¬¸ì¥ :  {result['cleaned']}")
                print(f"   â–¶ ì¹˜í™˜ ë‚´ì—­ :  {', '.join(result['replace_logs'])}")
            else:
                print(f"   â–¶ ìˆœí™” ë¬¸ì¥ :  (ë³€ë™ ì‚¬í•­ ì—†ìŒ)")

            # 3. ê°ì§€ ìœ í˜•
            # ë¦¬ìŠ¤íŠ¸ë¥¼ ë³´ê¸° ì¢‹ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
            labels_str = ", ".join(result['labels'])
            print(f"   â–¶ ê°ì§€ ìœ í˜• :  [{labels_str}]")

            # 4. AI í”¼ë“œë°±
            print(f"   â–¶ AI í”¼ë“œë°± :  \"{result['feedback']}\"")
            print("")  # ê³µë°± ë¼ì¸

        except KeyboardInterrupt:
            print("\n\n[ì‹œìŠ¤í…œ] ê°•ì œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\n[ì˜¤ë¥˜] ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


if __name__ == "__main__":
    run_demo()